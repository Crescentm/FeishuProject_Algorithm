{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c6668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guyue/Workspace/feishu_project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import BertTokenizerFast\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6137e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html(tokens, attention_weights, output_path=\"attention_avg.html\"):\n",
    "    norm_weights = (attention_weights - attention_weights.min()) / (attention_weights.max() - attention_weights.min() + 1e-8)\n",
    "    html = \"<html><head><style>span.token {padding:2px 5px;margin:2px;border-radius:5px;display:inline-block;font-family:monospace;}</style></head><body>\"\n",
    "    html += \"<h2>平均 Attention 可视化 ([CLS] → tokens)</h2><div>\"\n",
    "    for token, weight in zip(tokens, norm_weights):\n",
    "        red = int(255 * weight)\n",
    "        color = f\"rgba({red}, 0, 0, {weight:.2f})\"\n",
    "        html += f'<span class=\"token\" style=\"background-color:{color}\">{token}</span>'\n",
    "    html += \"</div></body></html>\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"✔️ 已保存 HTML 到: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcdea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weibo_text(text):\n",
    "    valid_keywords = [\n",
    "        '笑', '哭', '泪', '心', '怒', '汗', '抱', '喜欢', '亲', '色', '偷笑', '害羞', '惊讶', '开心', '哭泣',\n",
    "        '调皮', '害怕', '生气', '思考', '微笑', '呲牙', '委屈', '感动', '鼓掌', '加油', '抱拳', '拍手', '星星眼',\n",
    "        '晕', '晚安', '睡觉', '吐', '呆萌', '抓狂', '拍砖', '爱', '尴尬', '大哭', '坏笑', '高兴', '害羞', '发怒',\n",
    "        '兴奋', '酷', '赞', 'ok', '拜年', '卖萌', '抱抱', '转圈', '拜拜', '惊恐', '冷', '拜托', '拜谢', '炸裂',\n",
    "        '流汗', '偷乐', '开心', '傻眼', '鄙视', '叹气', '纠结', '疑问', '点赞', '赞', '抱歉', '感恩', '感冒', '感情',\n",
    "        '炸鸡', '雪人', '火', '狗', '猫', '熊', '兔', '猪', '骷髅', '鸡', '太阳', '月亮', '星', '花', '蛋糕',\n",
    "        '巧克力', '糖果', '礼物', '礼花', '福', '平安', '红包', '祝福', '祝', '祝贺', '新年', '节日', '节', '圣诞',\n",
    "        '生日', '万圣', '奥运', '火炬', '鼓掌', '加油', '胜利', '拥抱', '握手', '拳头', '挥手', '招手'\n",
    "    ]\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 去除 URL\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "\n",
    "    # 去除转发链 //@用户:\n",
    "    text = re.sub(r\"//@\\S+?:\", \"\", text)\n",
    "    # 去除正文中 @用户\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "    # 连续表情缩成一个\n",
    "    text = re.sub(r\"(\\[[^\\[\\]]+\\])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # 连续标点缩成一个\n",
    "    text = re.sub(r\"([！？!。，、,.，?])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # 利用 valid_keywords 过滤表情\n",
    "    def filter_emoji(match):\n",
    "        emoji = match.group(0)  # 带中括号的表情\n",
    "        content = emoji[1:-1]   # 去掉中括号\n",
    "        if any(keyword in content for keyword in valid_keywords):\n",
    "            return emoji\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    text = re.sub(r\"\\[[^\\[\\]]+\\]\", filter_emoji, text)\n",
    "\n",
    "    # 多空格替换成单空格，去首尾空格\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 去除无意义的词语\n",
    "    text = text.replace(\"转发微博\", \"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bb04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 3)\n",
      "预测类别: 积极\n",
      "attention shape: (6, 1, 8, 64, 64)\n",
      "✔️ 已保存 HTML 到: attention.html\n"
     ]
    }
   ],
   "source": [
    "text = \"高兴死了\"\n",
    "\n",
    "text = clean_weibo_text(text)\n",
    "\n",
    "id2label = {0: '积极', 1: '中性', 2: '消极'}\n",
    "# ========== 主推理 + 可视化流程 ==========\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"../minirbt-h256-with-emojis\")\n",
    "session = ort.InferenceSession(\"../model_cleaned/minirbt_with_attention_quant.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "attention_mask = inputs[\"attention_mask\"][0]\n",
    "seq_len = int(np.sum(attention_mask))\n",
    "\n",
    "# Run ONNX model\n",
    "input_names = [i.name for i in session.get_inputs()]\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "outputs = session.run(output_names, {k: inputs[k] for k in input_names})\n",
    "\n",
    "# Extract logits and attentions\n",
    "logits, attentions = outputs\n",
    "print(type(logits), logits.shape)\n",
    "pred = np.argmax(logits, axis=1)[0]\n",
    "print(f\"预测类别: {id2label[pred]}\")\n",
    "\n",
    "# Select attention: layer 5, head 0, [CLS] token (index 0)\n",
    "layer = 5  # 选择最后一层（MiniRBT 只有 6 层，索引0~5）\n",
    "\n",
    "# 提取该层所有 head 的 attention 矩阵\n",
    "all_heads = attentions[layer][0]  # shape: [num_heads, seq_len, seq_len]\n",
    "print(f\"attention shape: {np.array(attentions).shape}\")\n",
    "# 取平均\n",
    "avg_attention = np.mean(all_heads, axis=0)  # shape: [seq_len, seq_len]\n",
    "\n",
    "cls_attention = avg_attention[0][:seq_len]\n",
    "\n",
    "# Tokens without padding\n",
    "tokens = tokens[:seq_len]\n",
    "\n",
    "# Generate HTML file\n",
    "generate_html(tokens, cls_attention, output_path=\"attention.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feishu_project (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
